{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import california_housing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict, StratifiedKFold\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUsubgrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (input,) = ctx.saved_tensors\n",
    "        grad_input = torch.zeros_like(input)\n",
    "        \n",
    "        grad_input[input > 0] = grad_output[input > 0]\n",
    "        grad_input[input == 0] = grad_output[input == 0] * 0.5\n",
    "        return grad_input\n",
    "\n",
    "def relu_subgrad(x):\n",
    "    return ReLUsubgrad.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegReLUsubgrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(max=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (input,) = ctx.saved_tensors\n",
    "        grad_input = torch.zeros_like(input)\n",
    "\n",
    "        grad_input[input < 0] = grad_output[input < 0]\n",
    "        grad_input[input == 0] = grad_output[input == 0] * 0.5\n",
    "        return grad_input\n",
    "\n",
    "def neg_relu_subgrad(x):\n",
    "    return NegReLUsubgrad.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, init=\"looks_linear\"):\n",
    "        super(CustomLinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size, bias=True)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "        if init == \"zero\":\n",
    "            nn.init.zeros_(self.linear.weight)\n",
    "        elif init == \"looks_linear\":\n",
    "            if input_size * 2 != output_size:\n",
    "                raise ValueError(\"Output size must be twice that of input size\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                weight = torch.zeros(input_size * 2, input_size)\n",
    "\n",
    "                for i in range(self.linear.in_features):\n",
    "                    weight[2 * i, i] = 1\n",
    "                    weight[2 * i + 1, i] = -1\n",
    "\n",
    "                self.linear.weight.copy_(weight)\n",
    "                nn.init.zeros_(self.linear.bias)\n",
    "                \n",
    "            \"\"\" Example matrix: [\n",
    "                [1, 0, 0],\n",
    "                [-1, 0, 0],\n",
    "                [0, 1, 0],\n",
    "                [0, -1, 0],\n",
    "                [0, 0, 1],\n",
    "                [0, 0, -1]\n",
    "            ] \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showed some promising result in terms of generalization. Basic idea is to do gradient boosting on linear model first, then second model is second-order interaction between features, including a feature with itself, so n features require n * (n + 1) // 2 pairs, which are fed into the same amount of neurons where each neuron takes into two features, or a pair of features. This is, in constrast, to the traditional approach of feeding all features into each neuron, which enables neural network to do very high order feature interaction. Enabling higher-order feature interactions can be detrimental as higher-order feature interactions tend to be rarer and the network will be more likely to fit to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseNetwork(nn.Module):        \n",
    "    def __init__(self, input_size, output_size, is_first_model):\n",
    "        super(PairwiseNetwork, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.is_first_model = is_first_model\n",
    "                        \n",
    "        if is_first_model:\n",
    "            self.first_layer = CustomLinearLayer(input_size, output_size, init=\"zero\")\n",
    "        else:\n",
    "            num_unique_pairs = input_size * (input_size + 1) // 2\n",
    "            self.i_indices, self.j_indices = torch.triu_indices(input_size, input_size)\n",
    "\n",
    "            self.weights = nn.Parameter(torch.zeros(1, output_size, num_unique_pairs, 2, 2))\n",
    "            self.pairwise_biases = nn.Parameter(torch.zeros(output_size, num_unique_pairs, 2))\n",
    "            self.outputs_biases = nn.Parameter(torch.zeros(1, output_size))\n",
    "            \n",
    "    def forward(self, x, prev_output=None):\n",
    "        if self.is_first_model and prev_output is not None:\n",
    "            raise ValueError(\"This is the first model and prev_output is passed in\")\n",
    "        if not self.is_first_model and prev_output is None:\n",
    "            raise ValueError(\"This is not the first model and prev_output is not passed in\")\n",
    "        \n",
    "        if prev_output is None:\n",
    "            return self.first_layer(x)\n",
    "\n",
    "        x_i = x[:, self.i_indices]\n",
    "        x_j = x[:, self.j_indices]        \n",
    "        feature_pairs = torch.stack([x_i, x_j], dim=2).unsqueeze(1).unsqueeze(-1)\n",
    "\n",
    "        outputs = torch.matmul(self.weights, feature_pairs).squeeze(-1) + self.pairwise_biases\n",
    "        outputs = relu_subgrad(outputs)\n",
    "        outputs[:, :, :, 1] *= -1\n",
    "        return outputs.sum(dim=(2, 3)) + self.outputs_biases\n",
    "\n",
    "    def extract_features(self, x, prev_output=None):\n",
    "        if self.is_first_model:\n",
    "            raise ValueError(\"This is not intended for the first model\")\n",
    "        if not self.is_first_model and prev_output is None:\n",
    "            raise ValueError(\"This is not the first model and prev_output is not passed in\")\n",
    "            \n",
    "        return self.activation(self.first_layer(x))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
